{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    " Applying the Q-learning algorithm using Numpy and OpenAI Gym.\n",
    " \n",
    " [Reference](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - CartPole-v1\n",
    "\n",
    "This is a classic control problem implemented in the Gym package. From the docs: \n",
    "\n",
    ">A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "An example of the final control achieved is shown below.\n",
    "\n",
    "![CartPole Gif](https://github.com/Fernandohf/Reinforcement-Learning/blob/master/gym/results/result.gif?raw=true)\n",
    "\n",
    "Complete [list of environments](https://gym.openai.com/envs/#classic_control)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages:\n",
    "- **Gym**: Load environment.\n",
    "- **Numpy**: Efficient matricial operations.\n",
    "- **Seaborn**: Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment and get the states bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GYM environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min state values: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Max state values: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "# Observation bounds\n",
    "obs_low = env.observation_space.low\n",
    "obs_high = env.observation_space.high\n",
    "print(f\"Min state values: {obs_low}\")\n",
    "print(f\"Max state values: {obs_high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins to digitize position, angle, and angular velocity\n",
    "res1 = 50\n",
    "res2 = 50\n",
    "res3 = 20\n",
    "bins = [np.linspace(obs_low[0], obs_high[0], res1),\n",
    "        np.linspace(obs_low[2], obs_high[2], res2),\n",
    "        np.linspace(-10, 10, res3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function discretize the states variables\n",
    "def get_dstates(states, bins):\n",
    "    \"\"\"\n",
    "    Discretize only 3 state variables.\n",
    "    \"\"\"\n",
    "    d_states = []\n",
    "    c_states = [states[0], states[2], states[3]]\n",
    "    for i, s in enumerate(c_states):\n",
    "        d_states.append(np.digitize(s, bins[i]))\n",
    "    return tuple(d_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These states are 4 variables related to the cart:\n",
    "\n",
    "Index| State Variable\n",
    ":---:|:---\n",
    "0 |Cart Position\n",
    "1 |Cart Velocity\n",
    "2 |Pole Angle\n",
    "3 |Pole Velocity (at tip)\n",
    "\n",
    "In order to apply Q-Learning algorithm, we will discretize those variable. In order to train faster, we will ignore the **Cart Velocity** state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Number of actions and states\n",
    "action_size = env.action_space.n\n",
    "print(action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment allows 2 possible discrete actions:\n",
    "\n",
    "Index| Action Variable\n",
    ":---:|:---\n",
    "0 |Push Cart Left\n",
    "1 |Push Cart Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the q-table with the dimensions corresponding to each state variable discretization bucket and actions. Additionally, the hyperparameters are selected:\n",
    "\n",
    "- *Episodes*: Total number of episodes to fill q-table.\n",
    "- *Learning Rate*: Learning rate used in Bellman's Equation.\n",
    "- *Max Steps*: Max number of steps in each episode.\n",
    "- *Gamma*: Discounting rate for old rewards.\n",
    "- *Epsilon*: Initial Epsilon value, close to 1 represents exploration.\n",
    "- *Decay Rate*: Rate to reduce Epsilon value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 51, 21, 2)\n"
     ]
    }
   ],
   "source": [
    "# Initialize q-table\n",
    "qtable = np.zeros((res1 + 1, res2 + 1, res3 + 1, action_size))\n",
    "\n",
    "print(qtable.shape)\n",
    "# Hyperparameters\n",
    "total_episodes = 10000      # Total episodes\n",
    "learning_rate = 0.2         # Learning rate\n",
    "max_steps = 200             # Max steps per episode\n",
    "gamma = 0.95                # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0               # Exploration rate\n",
    "max_epsilon = 1.0           # Exploration probability at start\n",
    "min_epsilon = 0.1          # Minimum exploration probability \n",
    "decay_rate = 0.001         # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Table\n",
    "Now that everything is ready, the Q-table can start to be filled with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6092/10000 [01:32<01:13, 53.35it/s]"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "    # Reset the environment\n",
    "    states = env.reset()\n",
    "    dstates = get_dstates(states, bins)\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = np.random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[dstates])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_dstates = get_dstates(new_state, bins)\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        ind = dstates + (action,)\n",
    "        qtable[ind] = (qtable[ind] + \n",
    "                       learning_rate * (reward + gamma * np.max(qtable[new_dstates]) -\n",
    "                                        qtable[ind]))\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        dstates = new_dstates\n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Using the highest reward for each state on **Q-Table**, we can estimate the best action. The code below executes the simulation following the optimal actions trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final results\n",
    "# Wrapper to save  as video\n",
    "menv = gym.wrappers.Monitor(env, \"results\", force=True) # Uncomment to save video\n",
    "for episode in range(3):\n",
    "    states = menv.reset()\n",
    "    dstates = get_dstates(states, bins)\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(250):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[dstates])\n",
    "        \n",
    "        new_state, reward, done, info = menv.step(action)\n",
    "        new_dstates = get_dstates(new_state, bins)\n",
    "        ind = dstates + (action,)\n",
    "        menv.render()\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "        dstates = new_dstates\n",
    "    else:\n",
    "        menv.close()\n",
    "menv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Evolution\n",
    "\n",
    "The graph shows how the mean reward evolves during the Q-Table filling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "sns.lineplot(range(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Decay\n",
    "\n",
    "How the epsilon value decays with the number of epsiodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "x = np.arange(1, total_episodes)\n",
    "y = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*x) \n",
    "sns.lineplot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
