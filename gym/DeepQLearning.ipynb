{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    " Applying the Deep Q-learning algorithm using PyTorch and OpenAI Gym.\n",
    " \n",
    " [Reference](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    " \n",
    " FOLLOW:\n",
    " [REFERENCE UDACITY](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - CartPole-v1\n",
    "\n",
    "This is a classic control problem implemented in the Gym package. From the docs: \n",
    "\n",
    ">A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "An example of the final control achieved is shown below.\n",
    "\n",
    "![CartPole Gif](https://github.com/Fernandohf/Reinforcement-Learning/blob/master/gym/results/result.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages:\n",
    "- **Gym**: Load environment.\n",
    "- **Numpy**: Efficient matricial operations.\n",
    "- **PyTorch**: Deep learning framework.\n",
    "- **Seaborn**: Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment and get the states bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GYM environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Fully Connected Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of the deep fully conected network.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 256, bias=False)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc2 = nn.Linear(256, 64, bias=False)\n",
    "        self.fc3 = nn.Linear(64, outputs)\n",
    "        \n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \"\"\"\n",
    "    Class responsible to keep the Memory buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"\n",
    "        Creates the object\n",
    "        \n",
    "        Input:\n",
    "        max_size: Max number os experiences saved\n",
    "        \n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\"\n",
    "        Add an expericende tuple to memory\n",
    "        \n",
    "        Inputs: \n",
    "            experience: Tuple with all experience information\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Random sample from the memory\n",
    "        \n",
    "        Input:\n",
    "            batch_size: Sample size\n",
    "        \n",
    "        \"\"\"\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TOTAL_EPISODES = 32* 100\n",
    "MAX_STEPS = 200\n",
    "MEMORY_SIZE = 10000\n",
    "MEMORY_FILLER = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning on cpu\n"
     ]
    }
   ],
   "source": [
    "# Look for cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Traning on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Number of actions and states\n",
    "action_size = env.action_space.n\n",
    "print(action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Observation bounds\n",
    "obs_size = len(env.observation_space.sample())\n",
    "print(obs_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model\n",
    "model = DQN(obs_size, action_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion\n",
    "criterion = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "def init_memory(memory, episodes_fill=100, max_steps=200):\n",
    "    for episode in range(episodes_fill):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Choose random action a in the current world state (s)\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Take the action and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, new_state, done))\n",
    "            \n",
    "            # Define state\n",
    "            state = new_state\n",
    "            # If the episode is finished\n",
    "            if done:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_memory(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb47bd2baef445babb5d908a348dc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For life or until learning is stopped\n",
    "epsilon = EPS_START\n",
    "losses = []\n",
    "for episode in tqdm_notebook(range(TOTAL_EPISODES)):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = np.random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            with torch.no_grad():\n",
    "                # Pick action with the larger expected reward.\n",
    "                state_tensor = torch.from_numpy(state).view(1, -1).to(device).float()\n",
    "                action_tensor = torch.argmax(model(state_tensor), dim=1)\n",
    "                action = action_tensor.cpu().numpy().squeeze()\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store transition <st,at,rt+1,st+1> in memory D\n",
    "        memory.add((state, action, reward, new_state, done))\n",
    "                \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "    # Empty acumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get the batch from memory\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    rewards = np.array([each[2] for each in batch])\n",
    "    states_mb = np.array([each[0] for each in batch])\n",
    "    actions_mb = np.array([each[1] for each in batch])\n",
    "    rewards_mb = np.array([each[2] for each in batch]) \n",
    "    new_states_mb = np.array([each[3] for each in batch])\n",
    "    dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "    target_Qs_batch = []\n",
    "\n",
    "    # Get Q values for next_state\n",
    "    Qs_new_state = model(torch.from_numpy(new_states_mb).to(device).float())\n",
    "    # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "    for i in range(0, len(batch)):\n",
    "        terminal = dones_mb[i]\n",
    "\n",
    "        # If we are in a terminal state, only equals reward\n",
    "        if terminal:\n",
    "            target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "        else:\n",
    "            target = (torch.tensor(rewards_mb[i]) +\n",
    "                      torch.tensor(GAMMA) * torch.max(Qs_new_state[i]))\n",
    "            target_Qs_batch.append(target)\n",
    "\n",
    "    # Target scores\n",
    "    targets_mb = torch.tensor([each for each in target_Qs_batch]).to(device)\n",
    "    model_pred = model(torch.from_numpy(states_mb).to(device).float())\n",
    "    \n",
    "    # Get max q-values\n",
    "    q_pred = torch.max(model_pred, dim=1)[0]\n",
    "    \n",
    "    loss = criterion(q_pred.float(), targets_mb.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = EPS_END + (EPS_START - EPS_END) * np.exp(-EPS_DECAY * episode) \n",
    "    \n",
    "    # Saves loss\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the q-table with the dimensions corresponding to each state variable discretization bucket and actions. Additionally, the hyperparameters are selected:\n",
    "\n",
    "- *Episodes*: Total number of episodes to fill q-table.\n",
    "- *Learning Rate*: Learning rate used in Bellman's Equation.\n",
    "- *Max Steps*: Max number of steps in each episode.\n",
    "- *Gamma*: Discounting rate for old rewards.\n",
    "- *Epsilon*: Initial Epsilon value, close to 1 represents exploration.\n",
    "- *Decay Rate*: Rate to reduce Epsilon value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Using the highest reward for each state on **Q-Table**, we can estimate the best action. The code below executes the simulation following the optimal actions trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a8893f177c89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Show final results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Wrapper to save  as video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmenv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Uncomment to save video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# menv = env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# Show final results\n",
    "# Wrapper to save  as video\n",
    "menv = gym.wrappers.Monitor(env, \"results\", force=True) # Uncomment to save video\n",
    "# menv = env\n",
    "for episode in range(3):\n",
    "    states = menv.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(250):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        state_tensor = torch.from_numpy(states).view(1, -1).to(device).float()\n",
    "        action_tensor = torch.argmax(model(state_tensor), dim=1)\n",
    "        action = action_tensor.cpu().numpy().squeeze()\n",
    "\n",
    "        new_state, reward, done, info = menv.step(action)\n",
    "        menv.render()\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "    else:\n",
    "        menv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Evolution\n",
    "\n",
    "The graph shows how the mean reward evolves during the Q-Table filling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f15f51e4a20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rewards\n",
    "sns.lineplot(range(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Decay\n",
    "\n",
    "How the epsilon value decays with the number of epsiodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6cbfb92efb37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_epsilon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_epsilon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecay_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_episodes' is not defined"
     ]
    }
   ],
   "source": [
    "# Rewards\n",
    "x = np.arange(1, total_episodes)\n",
    "y = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*x) \n",
    "sns.lineplot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
